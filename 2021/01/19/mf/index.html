<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!--Description-->
    
        <meta name="description" content="引言
基础知识
求解过程
奇异值分解
梯度下降分解


消除用户和物品打分的偏差


小结




引言 针对经典的协同过滤算法的头部效应较明显、泛化能力较弱的问题，矩阵分解算法被提出。矩阵算法在协同过滤的基础上加入了隐向量的概念，加强了模型处理稀疏矩阵的能力，针对性的解决了协同过滤存在的主要问题。">
    

    <!--Author-->
    
        <meta name="author" content="Liyang Ye">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="矩阵分解"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Liyang&#39;s Blog"/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>矩阵分解 - Liyang&#39;s Blog</title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/sass/main.css">


    <!--[if lt IE 8]>
        
<script src="/js/ie/html5shiv.js"></script>

    <![endif]-->

    <!--[if lt IE 8]>
        
<link rel="stylesheet" href="/sass/ie8.css">

    <![endif]-->

    <!--[if lt IE 9]>
        
<link rel="stylesheet" href="/sass/ie9.css">

    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 5.3.0"></head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/logo.svg" alt="" /></span><span class="title">Liyang's Blog</span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">Menu</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>Menu</h2>
    <ul>
        
            <li>
                <a href="/">Home</a>
            </li>
        
            <li>
                <a href="/archives">Archives</a>
            </li>
        
            <li>
                <a href="/about.html">About</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1 class="title">矩阵分解</h1>
    <div class="meta">
        
        
            <span class="post-count">
           字数:
            <a href="">1.6k字 |</a>  
            </span>
        
        
            <span class="post-count">
           阅读时间:
            <a href="">6min |</a>  
            </span>
        
	<span class="post-count">发布于:2021-01-19</span>
    </div>


<!-- Tags -->


<div class="tags">
    <a href="/tags/矩阵分解/" class="button small">矩阵分解</a> <a href="/tags/SVD/" class="button small">SVD</a> <a href="/tags/梯度下降/" class="button small">梯度下降</a> <a href="/tags/奇异值分解/" class="button small">奇异值分解</a>
</div>




    <span class="image main"><img src="/images/mf/mf_logic_hero.png" alt="" /></span>


<!-- Gallery -->


<!-- Content -->
<div class="toc">

<!-- toc -->
<ul>
<li><a href="#yin-yan">引言</a></li>
<li><a href="#ji-chu-zhi-shi">基础知识</a><ul>
<li><a href="#qiu-jie-guo-cheng">求解过程</a><ul>
<li><a href="#qi-yi-zhi-fen-jie">奇异值分解</a></li>
<li><a href="#ti-du-xia-jiang-fen-jie">梯度下降分解</a></li>
</ul>
</li>
<li><a href="#xiao-chu-yong-hu-he-wu-pin-da-fen-de-pian-chai">消除用户和物品打分的偏差</a></li>
</ul>
</li>
<li><a href="#xiao-jie">小结</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="yin-yan">引言</span><a href="#yin-yan" class="header-anchor"> </a></h1><p>针对经典的协同过滤算法的头部效应较明显、泛化能力较弱的问题，矩阵分解算法被提出。矩阵算法在协同过滤的基础上加入了隐向量的概念，加强了模型处理稀疏矩阵的能力，针对性的解决了协同过滤存在的主要问题。</p>
<h1><span id="ji-chu-zhi-shi">基础知识</span><a href="#ji-chu-zhi-shi" class="header-anchor"> </a></h1><p>矩阵分解算法期望为每一个用户和物品生成一个隐变量，将用户和物品定位到隐向量的表示空间上，这样用户和物品可以在同一个二维空间中，只要给用户<strong>推荐距离相近</strong>的物品。在<code>矩阵分解</code>的算法框架下，用户和物品的<strong>隐向量是通过分解协同过滤生成的共现矩阵</strong>得到的。</p>
<p><img src="/images/mf/mf_logic.jpg" alt></p>
<p>矩阵分解算法将$m \times n$维的共现矩阵$R$分解为$m \times k$维的用户矩阵$U$和$k \times n$维的物品矩阵$V$相乘的形式。其中$m$是用户数量，$n$是物品数量，$k$是隐向量的维度。<strong>$k$的大小决定了隐向量表达能力的强弱</strong>。</p>
<p>基于用户矩阵$U$和物品矩阵$V$，用户$u$对物品$i$的预估评分：</p>
<script type="math/tex; mode=display">
\hat{r}_{ui}=q_i^Tp_u</script><p>其中$p_u$是用户$u$在用户矩阵$U$中的对应行向量，$q_i$是物品$i$在物品矩阵$V$中的对应列向量。</p>
<h2><span id="qiu-jie-guo-cheng">求解过程</span><a href="#qiu-jie-guo-cheng" class="header-anchor"> </a></h2><p>对矩阵进行矩阵分解的主要方法有三种：<strong>特征值分解</strong>（Eigen Decomposition）、<strong>奇异值分解</strong>（Singular Value Decomposition，<strong>SVD</strong>）和<strong>梯度下降</strong>（Gradient Descent）。其中，<strong>特征值分解只能用于方阵</strong>，不适用于分解用户-物品矩阵。</p>
<h3><span id="qi-yi-zhi-fen-jie">奇异值分解</span><a href="#qi-yi-zhi-fen-jie" class="header-anchor"> </a></h3><p>假设矩阵$M$是一个$m \times n$的矩阵，则一定存在一个分解$M=U  \sum{V^t}$，其中$U$是$m \times m$的正交矩阵，$V$是$n \times n$的正交矩阵, $\sum$是$m \times n$的对角阵。</p>
<p>取对角阵$\sum$中较大的$k$个元素作为隐含特征，删除$/sum$的其他维度及$U$和$V$中对应的维度，矩阵$M$被分解为$M \approx {U<em>{m \times k}\sum</em>{k \times k}V_{k \times n}}^T$，至此完成了隐向量维度为$k$的矩阵分解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linalg <span class="keyword">as</span> la</span><br><span class="line"><span class="comment">#numpy.linalg.svd实现svd分解</span></span><br><span class="line">A=np.random.randint(<span class="number">1</span>,<span class="number">25</span>,[<span class="number">5</span>,<span class="number">5</span>])</span><br><span class="line">u,sigma,vt = la.svd(A)</span><br><span class="line">print(A)</span><br><span class="line"><span class="comment">#验证 A = u·s·vt</span></span><br><span class="line">S = np.zeros([<span class="number">5</span>,<span class="number">5</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    S[i][i] = sigma[i]</span><br><span class="line">tmp = np.dot(u,S)</span><br><span class="line">print(np.dot(tmp,vt))</span><br><span class="line"></span><br><span class="line">[[ <span class="number">1</span> <span class="number">11</span> <span class="number">13</span>  <span class="number">1</span> <span class="number">16</span>]</span><br><span class="line"> [<span class="number">17</span>  <span class="number">8</span> <span class="number">16</span>  <span class="number">8</span> <span class="number">15</span>]</span><br><span class="line"> [<span class="number">17</span> <span class="number">10</span> <span class="number">22</span> <span class="number">12</span>  <span class="number">1</span>]</span><br><span class="line"> [<span class="number">23</span>  <span class="number">3</span> <span class="number">19</span> <span class="number">24</span> <span class="number">10</span>]</span><br><span class="line"> [ <span class="number">3</span> <span class="number">11</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">6</span>]]</span><br><span class="line">[[ <span class="number">1.</span> <span class="number">11.</span> <span class="number">13.</span>  <span class="number">1.</span> <span class="number">16.</span>]</span><br><span class="line"> [<span class="number">17.</span>  <span class="number">8.</span> <span class="number">16.</span>  <span class="number">8.</span> <span class="number">15.</span>]</span><br><span class="line"> [<span class="number">17.</span> <span class="number">10.</span> <span class="number">22.</span> <span class="number">12.</span>  <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">23.</span>  <span class="number">3.</span> <span class="number">19.</span> <span class="number">24.</span> <span class="number">10.</span>]</span><br><span class="line"> [ <span class="number">3.</span> <span class="number">11.</span>  <span class="number">3.</span>  <span class="number">7.</span>  <span class="number">6.</span>]]</span><br></pre></td></tr></table></figure>
<p>S为对角阵$\sum$中较大的$k$个元素作为隐含特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> S</span><br><span class="line">  </span><br><span class="line">[[<span class="number">60.10509337</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.</span>         <span class="number">20.54521797</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>         <span class="number">11.21739192</span>  <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">8.87411711</span>  <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">4.86743598</span>]]</span><br></pre></td></tr></table></figure>
<p>奇异值分解存在两个缺陷：</p>
<ol>
<li>奇异值分解要求原始的共现矩阵是稠密的。互联网场景下大部分用户的行为历史非常少，共现矩阵稀疏，如果要用奇异值分解，那么就<strong>需要对缺失元素进行填充</strong>。</li>
<li><strong>计算复杂度高</strong>，达到了$O(mn^2)$的级别。</li>
</ol>
<p>因此，梯度下降法成为了进行矩阵分解的主要方法。</p>
<h3><span id="ti-du-xia-jiang-fen-jie">梯度下降分解</span><a href="#ti-du-xia-jiang-fen-jie" class="header-anchor"> </a></h3><p>目标函数：</p>
<script type="math/tex; mode=display">
\min_{q^*,p^*}\sum_{(u,i) \in K}(r_{ui}-q_i^Tp_u)^2 + \lambda({\left|\left| q_i\right|\right|}^2 + {\left|\left|p_u\right|\right|}^2)</script><p>为了防止过拟合，加入加号后面的正则化项。该目标函数的目的是让原始评分$r_{ui}$与用户向量和物品向量之积$q_i^Tp_u$的差尽量小，这样才能<strong>最大限度的保存共现矩阵的原始信息</strong>。</p>
<p>梯度下降过程：</p>
<ol>
<li><p>确定目标函数</p>
</li>
<li><p>对目标函数求偏导，得到梯度下降的方向和幅度。</p>
<p>根据目标函数，对$q_i$求偏导，得到：</p>
<script type="math/tex; mode=display">
2(r_{ui}-q_i^Tp_u)p_u-2\lambda q_i</script><p>对$p_u$求偏导，得到：</p>
<script type="math/tex; mode=display">
2(r_{ui}-q_i^Tp_u)q_i-2\lambda p_u</script></li>
<li><p>利用第二步的求导结果，沿梯度的反方向更新参数：</p>
<script type="math/tex; mode=display">
q_i\gets q_i-\gamma((r_{ui}-q_i^Tp_u)p_u-\lambda q_i</script><script type="math/tex; mode=display">
p_u\gets p_u-\gamma((r_{ui}-q_i^Tp_u)q_i-\lambda p_u)</script><p>其中$\gamma$为学习率。</p>
</li>
<li><p>当迭代次数超过上限$n$或损失低于闸值$\theta$时，结束训练，否则循环第三步。</p>
</li>
</ol>
<h2><span id="xiao-chu-yong-hu-he-wu-pin-da-fen-de-pian-chai">消除用户和物品打分的偏差</span><a href="#xiao-chu-yong-hu-he-wu-pin-da-fen-de-pian-chai" class="header-anchor"> </a></h2><p>由于不同用户对打分系统的评估不同，在5分满分的评分里，有的人认为1分是最低的，有的人认为3分就是差评了。为了消除用户和物品打分的偏差（Bias），常用的做法是在矩阵分解时<strong>加入用户和物品的偏差向量</strong>：</p>
<script type="math/tex; mode=display">
r_{ui} = \mu + b_i + b_u + q_i^Tp_u</script><p>其中$\mu$是全局偏差常数，$b_i$时物品偏差系数，可使用物品$i$收到的所有评分的均值，$b_u$是用户偏差系数，可使用用户$u$给出的所有评分的均值。</p>
<p>目标函数需要修改为：</p>
<script type="math/tex; mode=display">
\min_{q^*,p^*,b^*}\sum_{(u,i) \in K}(r_{ui}-\mu-b_u-b_i-p_u^Tq_i)^2+\lambda(||p_u||^2+||q_i||^2+b_u^2+b_i^2)</script><p><strong>加入用户和物品的打分偏差项之后，矩阵分解得到的隐向量更能反映不同用户对不同物品的“真实“态度差异，更容易捕捉有价值信息，避免推荐有偏</strong>。</p>
<h1><span id="xiao-jie">小结</span><a href="#xiao-jie" class="header-anchor"> </a></h1><ul>
<li><p>由于隐向量的存在，使任意用户和物品之间都可以得到预测分值。</p>
</li>
<li><p>隐向量生成过程是对共现矩阵的全局拟合过程，隐向量是利用全局信息生成的，有更强的泛化能力。</p>
</li>
<li><p>泛化能力强。</p>
</li>
<li><p>空间复杂度低。$O(n+m)·k$</p>
</li>
<li><p>更好的扩展性和灵活性。与embedding思想相似，容易与其它特征拼接组合，可以和深度学习无缝结合。</p>
</li>
<li><p>矩阵分解不能加入用户、物品和上下文相关特征，推荐结果有局限性。</p>
</li>
</ul>



<!-- Comments -->
<div>
    
    <hr />
    <h3>Comments:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



</div>



    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });
      </script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>




            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <h2>About</h2>
            <div>
                This blog is used to save the things that I have learnt from Machine Learning and Deep Learning.
            </div>
        </section>
        <section>
            <h2>Follow</h2>
            <ul class="icons">
                
                
                
                
                
                    <li><a href="https://github.com/yeliyang/" class="icon style2 fa-github" target="_blank" ><span class="label">GitHub</span></a></li>
                
                
                
                
                
                    <li><a href="\#" class="icon style2 fa-envelope-o" target="_blank" ><span class="label">Email</span></a></li>
                
                
                    <li><a href="\#" class="icon style2 fa-rss" target="_blank" ><span class="label">RSS</span></a></li>
                
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; Untitled. All rights reserved</li>
            <li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
            <li>Hexo: <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- skel -->

<script src="/js/skel.min.js"></script>


<!-- Custom Code -->

<script src="/js/util.js"></script>


<!--[if lte IE 8]>

<script src="/js/ie/respond.min.js"></script>

<![endif]-->

<!-- Custom Code -->

<script src="/js/main.js"></script>


<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'liyang';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>


</body>

</html>