<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!--Description-->
    
        <meta name="description" content="å¼•è¨€
æƒé‡è¡°å‡
é«˜ç»´çº¿æ€§å›å½’å®éªŒ
ç®€æ´å®ç°


ä¸¢å¼ƒæ³•
è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹
ç®€æ´å®ç°




å°ç»“




å¼•è¨€ æ·±åº¦å­¦ä¹ ä¸­ç”±äºå±‚åº¦çš„å¢åŠ ï¼Œæ•°æ®é‡çš„è¿‡å°‘ï¼Œæ•°å­¦å…¬å¼é‡‡ç”¨ä¸‰é˜¶åŠä»¥ä¸Šçš„ç§ç§æƒ…å†µææ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚é˜²æ­¢è¿‡æ‹Ÿåˆæ˜¯æ·±åº¦å­¦ä¹ ä¸å¯é¿å…çš„å¿…ä¿®è¯¾ï¼Œç›®å‰ä¸»æµçš„æœ‰ä¸¤ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼š1.æƒé‡è¡°å‡ã€‚2.ä¸¢å¼ƒæ³•ã€‚
æƒ">
    

    <!--Author-->
    
        <meta name="author" content="Liyang Ye">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="é˜²æ­¢è¿‡æ‹Ÿåˆ"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Liyang&#39;s Blog"/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>é˜²æ­¢è¿‡æ‹Ÿåˆ - Liyang&#39;s Blog</title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/sass/main.css">


    <!--[if lt IE 8]>
        
<script src="/js/ie/html5shiv.js"></script>

    <![endif]-->

    <!--[if lt IE 8]>
        
<link rel="stylesheet" href="/sass/ie8.css">

    <![endif]-->

    <!--[if lt IE 9]>
        
<link rel="stylesheet" href="/sass/ie9.css">

    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 5.3.0"></head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/logo.svg" alt="" /></span><span class="title">Liyang's Blog</span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">Menu</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>Menu</h2>
    <ul>
        
            <li>
                <a href="/">Home</a>
            </li>
        
            <li>
                <a href="/archives">Archives</a>
            </li>
        
            <li>
                <a href="/about.html">About</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1 class="title">é˜²æ­¢è¿‡æ‹Ÿåˆ</h1>
    <div class="meta">
        
        
            <span class="post-count">
           å­—æ•°:
            <a href="">2.9kå­— |</a>  
            </span>
        
        
            <span class="post-count">
           é˜…è¯»æ—¶é—´:
            <a href="">12min |</a>  
            </span>
        
	<span class="post-count">å‘å¸ƒäº:2021-01-17</span>
    </div>


<!-- Tags -->


<div class="tags">
    <a href="/tags/overfit/" class="button small">overfit</a> <a href="/tags/dropout/" class="button small">dropout</a> <a href="/tags/æƒé‡è¡°å‡/" class="button small">æƒé‡è¡°å‡</a>
</div>




    <span class="image main"><img src="/images/overfit/overfit_hero.jpeg" alt="" /></span>


<!-- Gallery -->


<!-- Content -->
<div class="toc">

<!-- toc -->
<ul>
<li><a href="#yin-yan">å¼•è¨€</a></li>
<li><a href="#quan-chong-shuai-jian">æƒé‡è¡°å‡</a></li>
<li><a href="#gao-wei-xian-xing-hui-gui-shi-yan">é«˜ç»´çº¿æ€§å›å½’å®éªŒ</a><ul>
<li><a href="#jian-ji-shi-xian">ç®€æ´å®ç°</a></li>
</ul>
</li>
<li><a href="#diu-qi-fa">ä¸¢å¼ƒæ³•</a><ul>
<li><a href="#xun-lian-he-ce-shi-mo-xing">è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹</a><ul>
<li><a href="#jian-ji-shi-xian-1">ç®€æ´å®ç°</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#xiao-jie">å°ç»“</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="yin-yan">å¼•è¨€</span><a href="#yin-yan" class="header-anchor"> </a></h1><p>æ·±åº¦å­¦ä¹ ä¸­ç”±äºå±‚åº¦çš„å¢åŠ ï¼Œæ•°æ®é‡çš„è¿‡å°‘ï¼Œæ•°å­¦å…¬å¼é‡‡ç”¨ä¸‰é˜¶åŠä»¥ä¸Šçš„ç§ç§æƒ…å†µææ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚é˜²æ­¢è¿‡æ‹Ÿåˆæ˜¯æ·±åº¦å­¦ä¹ ä¸å¯é¿å…çš„å¿…ä¿®è¯¾ï¼Œç›®å‰ä¸»æµçš„æœ‰ä¸¤ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼š1.æƒé‡è¡°å‡ã€‚2.ä¸¢å¼ƒæ³•ã€‚</p>
<h1><span id="quan-chong-shuai-jian">æƒé‡è¡°å‡</span><a href="#quan-chong-shuai-jian" class="header-anchor"> </a></h1><p>æƒé‡è¡°å‡ï¼ˆweight decayï¼‰ç­‰ä»·äº$L_2$èŒƒæ•°æ­£åˆ™åŒ–ï¼ˆregularizationï¼‰ã€‚æ­£åˆ™åŒ–é€šè¿‡<strong>ä¸ºæ¨¡å‹æŸå¤±å‡½æ•°æ·»åŠ æƒ©ç½šé¡¹ä½¿å­¦å‡ºçš„æ¨¡å‹å‚æ•°å€¼è¾ƒå°</strong>ï¼Œæ˜¯åº”å¯¹è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ‰‹æ®µã€‚æˆ‘ä»¬å…ˆæè¿°$L_2$èŒƒæ•°æ­£åˆ™åŒ–ï¼Œå†è§£é‡Šå®ƒä¸ºä½•åˆç§°æƒé‡è¡°å‡ã€‚</p>
<p>$L_2$èŒƒæ•°æ­£åˆ™åŒ–åœ¨æ¨¡å‹åŸæŸå¤±å‡½æ•°åŸºç¡€ä¸Šæ·»åŠ $L_2$èŒƒæ•°æƒ©ç½šé¡¹ï¼Œä»è€Œå¾—åˆ°è®­ç»ƒæ‰€éœ€è¦æœ€å°åŒ–çš„å‡½æ•°ã€‚$L_2$èŒƒæ•°æƒ©ç½šé¡¹æŒ‡çš„æ˜¯æ¨¡å‹æƒé‡å‚æ•°æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹å’Œä¸ä¸€ä¸ªæ­£çš„å¸¸æ•°çš„ä¹˜ç§¯ã€‚ä»¥çº¿æ€§å›å½’æŸå¤±å‡½æ•°</p>
<script type="math/tex; mode=display">
\ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2</script><p>ä¸ºä¾‹ï¼Œå…¶ä¸­$w_1, w_2$æ˜¯<strong>æƒé‡å‚æ•°</strong>ï¼Œ$b$æ˜¯<strong>åå·®å‚æ•°</strong>ï¼Œæ ·æœ¬$i$çš„è¾“å…¥ä¸º$x_1^{(i)}, x_2^{(i)}$ï¼Œæ ‡ç­¾ä¸º$y^{(i)}$ï¼Œæ ·æœ¬æ•°ä¸º$n$ã€‚å°†æƒé‡å‚æ•°ç”¨å‘é‡$\boldsymbol{w} = [w_1, w_2]$è¡¨ç¤ºï¼Œå¸¦æœ‰$L_2$èŒƒæ•°æƒ©ç½šé¡¹çš„æ–°æŸå¤±å‡½æ•°ä¸º</p>
<script type="math/tex; mode=display">\ell(w_1, w_2, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2,</script><p>å…¶ä¸­è¶…å‚æ•°$\lambda &gt; 0$ã€‚å½“æƒé‡å‚æ•°å‡ä¸º0æ—¶ï¼Œæƒ©ç½šé¡¹æœ€å°ã€‚å½“$\lambda$è¾ƒå¤§æ—¶ï¼Œæƒ©ç½šé¡¹åœ¨æŸå¤±å‡½æ•°ä¸­çš„æ¯”é‡è¾ƒå¤§ï¼Œè¿™é€šå¸¸ä¼šä½¿å­¦åˆ°çš„æƒé‡å‚æ•°çš„å…ƒç´ è¾ƒæ¥è¿‘0ã€‚å½“$\lambda$è®¾ä¸º0æ—¶ï¼Œæƒ©ç½šé¡¹å®Œå…¨ä¸èµ·ä½œç”¨ã€‚ä¸Šå¼ä¸­$L_2$èŒƒæ•°å¹³æ–¹$|\boldsymbol{w}|^2$å±•å¼€åå¾—åˆ°$w_1^2 + w_2^2$ã€‚æœ‰äº†$L_2$èŒƒæ•°æƒ©ç½šé¡¹åï¼Œåœ¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡$w_1$å’Œ$w_2$çš„è¿­ä»£æ–¹å¼æ›´æ”¹ä¸º</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).
\end{aligned}</script><p>å¯è§ï¼Œ$L_2$èŒƒæ•°æ­£åˆ™åŒ–ä»¤æƒé‡$w_1$å’Œ$w_2$å…ˆè‡ªä¹˜å°äº1çš„æ•°ï¼Œå†å‡å»ä¸å«æƒ©ç½šé¡¹çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œ$L_2$èŒƒæ•°æ­£åˆ™åŒ–åˆå«æƒé‡è¡°å‡ã€‚<strong>æƒé‡è¡°å‡é€šè¿‡æƒ©ç½šç»å¯¹å€¼è¾ƒå¤§çš„æ¨¡å‹å‚æ•°ä¸ºéœ€è¦å­¦ä¹ çš„æ¨¡å‹å¢åŠ äº†é™åˆ¶</strong>ï¼Œæ¥è¾¾åˆ°é˜²æ­¢è¿‡æ‹Ÿåˆçš„ä½œç”¨ã€‚å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬æœ‰æ—¶ä¹Ÿåœ¨æƒ©ç½šé¡¹ä¸­æ·»åŠ <strong>åå·®å…ƒç´ çš„å¹³æ–¹å’Œ</strong>ã€‚</p>
<h1><span id="gao-wei-xian-xing-hui-gui-shi-yan">é«˜ç»´çº¿æ€§å›å½’å®éªŒ</span><a href="#gao-wei-xian-xing-hui-gui-shi-yan" class="header-anchor"> </a></h1><p>ä¸‹é¢ï¼Œæˆ‘ä»¬ä»¥é«˜ç»´çº¿æ€§å›å½’ä¸ºä¾‹æ¥å¼•å…¥ä¸€ä¸ªè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶ä½¿ç”¨æƒé‡è¡°å‡æ¥åº”å¯¹è¿‡æ‹Ÿåˆã€‚è®¾æ•°æ®æ ·æœ¬ç‰¹å¾çš„ç»´åº¦ä¸º$p$ã€‚å¯¹äºè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ä¸­ç‰¹å¾ä¸º$x_1, x_2, \ldots, x_p$çš„ä»»ä¸€æ ·æœ¬ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹çš„çº¿æ€§å‡½æ•°æ¥ç”Ÿæˆè¯¥æ ·æœ¬çš„æ ‡ç­¾ï¼š</p>
<script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^p 0.01x_i +  \epsilon,</script><p>å…¶ä¸­å™ªå£°é¡¹$\epsilon$æœä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒã€‚ä¸ºäº†è¾ƒå®¹æ˜“åœ°è§‚å¯Ÿè¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬è€ƒè™‘é«˜ç»´çº¿æ€§å›å½’é—®é¢˜ï¼Œå¦‚è®¾ç»´åº¦$p=200$ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬ç‰¹æ„æŠŠè®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬æ•°è®¾ä½ï¼Œå¦‚20ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata, loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"></span><br><span class="line">n_train, n_test, num_inputs = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span></span><br><span class="line">true_w, true_b = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line"></span><br><span class="line">features = nd.random.normal(shape=(n_train + n_test, num_inputs))</span><br><span class="line">labels = nd.dot(features, true_w) + true_b</span><br><span class="line">labels += nd.random.normal(scale=<span class="number">0.01</span>, shape=labels.shape)</span><br><span class="line">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class="line">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br></pre></td></tr></table></figure>
<p>é¦–å…ˆï¼Œå®šä¹‰éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä¸ºæ¯ä¸ªå‚æ•°éƒ½é™„ä¸Šæ¢¯åº¦ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span>():</span></span><br><span class="line">    w = nd.random.normal(scale=<span class="number">1</span>, shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">    b = nd.zeros(shape=(<span class="number">1</span>,))</span><br><span class="line">    w.attach_grad()</span><br><span class="line">    b.attach_grad()</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢å®šä¹‰$L_2$èŒƒæ•°æƒ©ç½šé¡¹ã€‚è¿™é‡Œåªæƒ©ç½šæ¨¡å‹çš„æƒé‡å‚æ•°ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span>(<span class="params">w</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).<span class="built_in">sum</span>() / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_epochs, lr = <span class="number">1</span>, <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">net, loss = d2l.linreg, d2l.squared_loss</span><br><span class="line">train_iter = gdata.DataLoader(gdata.ArrayDataset(</span><br><span class="line">    train_features, train_labels), batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span>(<span class="params">lambd</span>):</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                <span class="comment"># æ·»åŠ äº†L2èŒƒæ•°æƒ©ç½šé¡¹</span></span><br><span class="line">                l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features, w, b),</span><br><span class="line">                             train_labels).mean().asscalar())</span><br><span class="line">        test_ls.append(loss(net(test_features, w, b),</span><br><span class="line">                            test_labels).mean().asscalar())</span><br><span class="line">    d2l.semilogy(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                 <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    print(<span class="string">&#x27;L2 norm of w:&#x27;</span>, w.norm().asscalar())</span><br></pre></td></tr></table></figure>
<p>å½“<code>lambd</code>è®¾ä¸º0æ—¶ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨æƒé‡è¡°å‡ã€‚ç»“æœè®­ç»ƒè¯¯å·®è¿œå°äºæµ‹è¯•é›†ä¸Šçš„è¯¯å·®ã€‚è¿™æ˜¯å…¸å‹çš„è¿‡æ‹Ÿåˆç°è±¡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit_and_plot(lambd=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/overfit/output_10_0.svg" alt></p>
<p>ä½¿ç”¨æƒé‡è¡°å‡åã€‚å¯ä»¥çœ‹å‡ºï¼Œè®­ç»ƒè¯¯å·®è™½ç„¶æœ‰æ‰€æé«˜ï¼Œä½†æµ‹è¯•é›†ä¸Šçš„è¯¯å·®æœ‰æ‰€ä¸‹é™ã€‚è¿‡æ‹Ÿåˆç°è±¡å¾—åˆ°ä¸€å®šç¨‹åº¦çš„ç¼“è§£ã€‚å¦å¤–ï¼Œæƒé‡å‚æ•°çš„ğ¿2L2èŒƒæ•°æ¯”ä¸ä½¿ç”¨æƒé‡è¡°å‡æ—¶çš„æ›´å°ï¼Œæ­¤æ—¶çš„æƒé‡å‚æ•°æ›´æ¥è¿‘0ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit_and_plot(lambd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/overfit/output_12_0.svg" alt></p>
<h2><span id="jian-ji-shi-xian">ç®€æ´å®ç°</span><a href="#jian-ji-shi-xian" class="header-anchor"> </a></h2><p>è¿™é‡Œæˆ‘ä»¬ç›´æ¥åœ¨æ„é€ <code>Trainer</code>å®ä¾‹æ—¶é€šè¿‡<code>wd</code>å‚æ•°æ¥æŒ‡å®šæƒé‡è¡°å‡è¶…å‚æ•°ã€‚é»˜è®¤ä¸‹ï¼ŒGluonä¼šå¯¹æƒé‡å’Œåå·®åŒæ—¶è¡°å‡ã€‚æˆ‘ä»¬å¯ä»¥åˆ†åˆ«å¯¹æƒé‡å’Œåå·®æ„é€ <code>Trainer</code>å®ä¾‹ï¼Œä»è€Œåªå¯¹æƒé‡è¡°å‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_gluon</span>(<span class="params">wd</span>):</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    net.add(nn.Dense(<span class="number">1</span>))</span><br><span class="line">    net.initialize(init.Normal(sigma=<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># å¯¹æƒé‡å‚æ•°è¡°å‡ã€‚æƒé‡åç§°ä¸€èˆ¬æ˜¯ä»¥weightç»“å°¾</span></span><br><span class="line">    trainer_w = gluon.Trainer(net.collect_params(<span class="string">&#x27;.*weight&#x27;</span>), <span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">                              &#123;<span class="string">&#x27;learning_rate&#x27;</span>: lr, <span class="string">&#x27;wd&#x27;</span>: wd&#125;)</span><br><span class="line">    <span class="comment"># ä¸å¯¹åå·®å‚æ•°è¡°å‡ã€‚åå·®åç§°ä¸€èˆ¬æ˜¯ä»¥biasç»“å°¾</span></span><br><span class="line">    trainer_b = gluon.Trainer(net.collect_params(<span class="string">&#x27;.*bias&#x27;</span>), <span class="string">&#x27;sgd&#x27;</span>,</span><br><span class="line">                              &#123;<span class="string">&#x27;learning_rate&#x27;</span>: lr&#125;)</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="comment"># å¯¹ä¸¤ä¸ªTrainerå®ä¾‹åˆ†åˆ«è°ƒç”¨stepå‡½æ•°ï¼Œä»è€Œåˆ†åˆ«æ›´æ–°æƒé‡å’Œåå·®</span></span><br><span class="line">            trainer_w.step(batch_size)</span><br><span class="line">            trainer_b.step(batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features),</span><br><span class="line">                             train_labels).mean().asscalar())</span><br><span class="line">        test_ls.append(loss(net(test_features),</span><br><span class="line">                            test_labels).mean().asscalar())</span><br><span class="line">    <span class="comment"># plot X and log(y)</span></span><br><span class="line">    d2l.semilogy(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                 <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    print(<span class="string">&#x27;L2 norm of w:&#x27;</span>, net[<span class="number">0</span>].weight.data().norm().asscalar())</span><br></pre></td></tr></table></figure>
<h1><span id="diu-qi-fa">ä¸¢å¼ƒæ³•</span><a href="#diu-qi-fa" class="header-anchor"> </a></h1><p>æ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸å¸¸ä½¿ç”¨ä¸¢å¼ƒæ³•ï¼ˆdropoutï¼‰ æ¥åº”å¯¹è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¸¢å¼ƒæ³•æœ‰ä¸€äº›ä¸åŒçš„å˜ä½“ã€‚æœ¬èŠ‚ä¸­æåˆ°çš„ä¸¢å¼ƒæ³•ç‰¹æŒ‡<strong>å€’ç½®ä¸¢å¼ƒæ³•</strong>ï¼ˆinverted dropoutï¼‰ã€‚</p>
<p>ä¸€ä¸ªå•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºã€‚å…¶ä¸­è¾“å…¥ä¸ªæ•°ä¸º4ï¼Œéšè—å•å…ƒä¸ªæ•°ä¸º5ï¼Œä¸”éšè—å•å…ƒ$h_i$ï¼ˆ$i=1, \ldots, 5$ï¼‰çš„è®¡ç®—è¡¨è¾¾å¼ä¸º</p>
<script type="math/tex; mode=display">h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right),</script><p>è¿™é‡Œ$\phi$æ˜¯æ¿€æ´»å‡½æ•°ï¼Œ$x<em>1, \ldots, x_4$æ˜¯è¾“å…¥ï¼Œéšè—å•å…ƒ$i$çš„æƒé‡å‚æ•°ä¸º$w</em>{1i}, \ldots, w_{4i}$ï¼Œåå·®å‚æ•°ä¸º$b_i$ã€‚å½“å¯¹è¯¥éšè—å±‚ä½¿ç”¨ä¸¢å¼ƒæ³•æ—¶ï¼Œ<strong>è¯¥å±‚çš„éšè—å•å…ƒå°†æœ‰ä¸€å®šæ¦‚ç‡è¢«ä¸¢å¼ƒæ‰</strong>ã€‚è®¾ä¸¢å¼ƒæ¦‚ç‡ä¸º$p$ï¼Œ<br>é‚£ä¹ˆæœ‰$p$çš„æ¦‚ç‡$h_i$ä¼šè¢«æ¸…é›¶ï¼Œæœ‰$1-p$çš„æ¦‚ç‡$h_i$ä¼šé™¤ä»¥$1-p$åšæ‹‰ä¼¸ã€‚ä¸¢å¼ƒæ¦‚ç‡æ˜¯ä¸¢å¼ƒæ³•çš„è¶…å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾éšæœºå˜é‡$\xi_i$ä¸º0å’Œ1çš„æ¦‚ç‡åˆ†åˆ«ä¸º$p$å’Œ$1-p$ã€‚ä½¿ç”¨ä¸¢å¼ƒæ³•æ—¶æˆ‘ä»¬è®¡ç®—æ–°çš„éšè—å•å…ƒ$h_iâ€™$</p>
<script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1-p} h_i.</script><p>ç”±äº$E(\xi_i) = 1-p$ï¼Œå› æ­¤</p>
<script type="math/tex; mode=display">E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i.</script><p>å³<strong>ä¸¢å¼ƒæ³•ä¸æ”¹å˜å…¶è¾“å…¥çš„æœŸæœ›å€¼</strong>ã€‚è®©æˆ‘ä»¬å¯¹å›¾3.3ä¸­çš„éšè—å±‚ä½¿ç”¨ä¸¢å¼ƒæ³•ï¼Œä¸€ç§å¯èƒ½çš„ç»“æœå¦‚å›¾3.5æ‰€ç¤ºï¼Œå…¶ä¸­$h_2$å’Œ$h_5$è¢«æ¸…é›¶ã€‚è¿™æ—¶è¾“å‡ºå€¼çš„è®¡ç®—ä¸å†ä¾èµ–$h_2$å’Œ$h_5$ï¼Œåœ¨åå‘ä¼ æ’­æ—¶ï¼Œä¸è¿™ä¸¤ä¸ªéšè—å•å…ƒç›¸å…³çš„æƒé‡çš„æ¢¯åº¦å‡ä¸º0ã€‚ç”±äºåœ¨è®­ç»ƒä¸­éšè—å±‚ç¥ç»å…ƒçš„ä¸¢å¼ƒæ˜¯éšæœºçš„ï¼Œå³$h_1, \ldots, h_5$éƒ½æœ‰å¯èƒ½è¢«æ¸…é›¶ï¼Œ<strong>è¾“å‡ºå±‚çš„è®¡ç®—æ— æ³•è¿‡åº¦ä¾èµ–$h_1, \ldots, h_5$ä¸­çš„ä»»ä¸€ä¸ª</strong>ï¼Œä»è€Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶<strong>èµ·åˆ°æ­£åˆ™åŒ–çš„ä½œç”¨</strong>ï¼Œå¹¶å¯ä»¥ç”¨æ¥åº”å¯¹è¿‡æ‹Ÿåˆã€‚åœ¨<strong>æµ‹è¯•æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¸ºäº†æ‹¿åˆ°æ›´åŠ ç¡®å®šæ€§çš„ç»“æœï¼Œä¸€èˆ¬ä¸ä½¿ç”¨ä¸¢å¼ƒæ³•</strong>ã€‚</p>
<p><img src="/images/overfit/dropout.svg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span>(<span class="params">X, drop_prob</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= drop_prob &lt;= <span class="number">1</span></span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    <span class="comment"># è¿™ç§æƒ…å†µä¸‹æŠŠå…¨éƒ¨å…ƒç´ éƒ½ä¸¢å¼ƒ</span></span><br><span class="line">    <span class="keyword">if</span> keep_prob == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X.zeros_like()</span><br><span class="line">    mask = nd.random.uniform(<span class="number">0</span>, <span class="number">1</span>, X.shape) &lt; keep_prob</span><br><span class="line">    <span class="keyword">return</span> mask * X / keep_prob</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">X = nd.arange(<span class="number">16</span>).reshape((<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line">dropout(X, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>  <span class="number">7.</span>]</span><br><span class="line"> [ <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span> <span class="number">13.</span> <span class="number">14.</span> <span class="number">15.</span>]]</span><br><span class="line">&lt;NDArray 2x8 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"></span><br><span class="line">dropout(X, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0.</span>  <span class="number">2.</span>  <span class="number">4.</span>  <span class="number">6.</span>  <span class="number">0.</span>  <span class="number">0.</span>  <span class="number">0.</span> <span class="number">14.</span>]</span><br><span class="line"> [ <span class="number">0.</span> <span class="number">18.</span>  <span class="number">0.</span>  <span class="number">0.</span> <span class="number">24.</span> <span class="number">26.</span> <span class="number">28.</span>  <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray 2x8 @cpu(<span class="number">0</span>)&gt;</span><br><span class="line"></span><br><span class="line">dropout(X, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">&lt;NDArray 2x8 @cpu(<span class="number">0</span>)&gt;</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬ä½¿ç”¨Fashion-MNISTæ•°æ®é›†ã€‚æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªéšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œå…¶ä¸­ä¸¤ä¸ªéšè—å±‚çš„è¾“å‡ºä¸ªæ•°éƒ½æ˜¯256ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_inputs, num_hiddens1))</span><br><span class="line">b1 = nd.zeros(num_hiddens1)</span><br><span class="line">W2 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_hiddens1, num_hiddens2))</span><br><span class="line">b2 = nd.zeros(num_hiddens2)</span><br><span class="line">W3 = nd.random.normal(scale=<span class="number">0.01</span>, shape=(num_hiddens2, num_outputs))</span><br><span class="line">b3 = nd.zeros(num_outputs)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢å®šä¹‰çš„æ¨¡å‹å°†å…¨è¿æ¥å±‚å’Œæ¿€æ´»å‡½æ•°ReLUä¸²èµ·æ¥ï¼Œå¹¶å¯¹æ¯ä¸ªæ¿€æ´»å‡½æ•°çš„è¾“å‡ºä½¿ç”¨ä¸¢å¼ƒæ³•ã€‚æˆ‘ä»¬å¯ä»¥åˆ†åˆ«è®¾ç½®å„ä¸ªå±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚é€šå¸¸çš„å»ºè®®æ˜¯æŠŠ<strong>é è¿‘è¾“å…¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡è®¾å¾—å°ä¸€ç‚¹</strong>ã€‚åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬æŠŠç¬¬ä¸€ä¸ªéšè—å±‚çš„ä¸¢å¼ƒæ¦‚ç‡è®¾ä¸º0.2ï¼ŒæŠŠç¬¬äºŒä¸ªéšè—å±‚çš„ä¸¢å¼ƒæ¦‚ç‡è®¾ä¸º0.5ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X</span>):</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H1 = (nd.dot(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> autograd.is_training():  <span class="comment"># åªåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨ä¸¢å¼ƒæ³•</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># åœ¨ç¬¬ä¸€å±‚å…¨è¿æ¥åæ·»åŠ ä¸¢å¼ƒå±‚</span></span><br><span class="line">    H2 = (nd.dot(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> autograd.is_training():</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># åœ¨ç¬¬äºŒå±‚å…¨è¿æ¥åæ·»åŠ ä¸¢å¼ƒå±‚</span></span><br><span class="line">    <span class="keyword">return</span> nd.dot(H2, W3) + b3</span><br></pre></td></tr></table></figure>
<h3><span id="xun-lian-he-ce-shi-mo-xing">è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹</span><a href="#xun-lian-he-ce-shi-mo-xing" class="header-anchor"> </a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,</span><br><span class="line">              params, lr)</span><br><span class="line"></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">1.2550</span>, train acc <span class="number">0.519</span>, test acc <span class="number">0.766</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.6153</span>, train acc <span class="number">0.771</span>, test acc <span class="number">0.825</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.5646</span>, train acc <span class="number">0.798</span>, test acc <span class="number">0.830</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.4765</span>, train acc <span class="number">0.826</span>, test acc <span class="number">0.855</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.4496</span>, train acc <span class="number">0.835</span>, test acc <span class="number">0.862</span></span><br></pre></td></tr></table></figure>
<h2><span id="jian-ji-shi-xian">ç®€æ´å®ç°</span><a href="#jian-ji-shi-xian" class="header-anchor"> </a></h2><p>åœ¨Gluonä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨å…¨è¿æ¥å±‚åæ·»åŠ <code>Dropout</code>å±‚å¹¶æŒ‡å®šä¸¢å¼ƒæ¦‚ç‡ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œ<code>Dropout</code>å±‚å°†ä»¥æŒ‡å®šçš„ä¸¢å¼ƒæ¦‚ç‡éšæœºä¸¢å¼ƒä¸Šä¸€å±‚çš„è¾“å‡ºå…ƒç´ ï¼›åœ¨æµ‹è¯•æ¨¡å‹æ—¶ï¼Œ<code>Dropout</code>å±‚å¹¶ä¸å‘æŒ¥ä½œç”¨ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">        nn.Dropout(drop_prob1),  <span class="comment"># åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚åæ·»åŠ ä¸¢å¼ƒå±‚</span></span><br><span class="line">        nn.Dense(<span class="number">256</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">        nn.Dropout(drop_prob2),  <span class="comment"># åœ¨ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚åæ·»åŠ ä¸¢å¼ƒå±‚</span></span><br><span class="line">        nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize(init.Normal(sigma=<span class="number">0.01</span>))</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">&#x27;sgd&#x27;</span>, &#123;<span class="string">&#x27;learning_rate&#x27;</span>: lr&#125;)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>,</span><br><span class="line">              <span class="literal">None</span>, trainer)</span><br><span class="line"></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">1.1450</span>, train acc <span class="number">0.553</span>, test acc <span class="number">0.791</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.5792</span>, train acc <span class="number">0.786</span>, test acc <span class="number">0.832</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.4875</span>, train acc <span class="number">0.820</span>, test acc <span class="number">0.847</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.4436</span>, train acc <span class="number">0.839</span>, test acc <span class="number">0.858</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.4174</span>, train acc <span class="number">0.848</span>, test acc <span class="number">0.867</span></span><br></pre></td></tr></table></figure>
<h1><span id="xiao-jie">å°ç»“</span><a href="#xiao-jie" class="header-anchor"> </a></h1><ul>
<li>æ­£åˆ™åŒ–é€šè¿‡ä¸ºæ¨¡å‹æŸå¤±å‡½æ•°æ·»åŠ æƒ©ç½šé¡¹ä½¿å­¦å‡ºçš„æ¨¡å‹å‚æ•°å€¼è¾ƒå°ï¼Œæ˜¯åº”å¯¹è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ‰‹æ®µã€‚</li>
<li>æƒé‡è¡°å‡ç­‰ä»·äºğ¿2èŒƒæ•°æ­£åˆ™åŒ–ï¼Œé€šå¸¸ä¼šä½¿å­¦åˆ°çš„æƒé‡å‚æ•°çš„å…ƒç´ è¾ƒæ¥è¿‘0ã€‚</li>
<li>æƒé‡è¡°å‡å¯ä»¥é€šè¿‡Gluonçš„<code>wd</code>è¶…å‚æ•°æ¥æŒ‡å®šã€‚</li>
<li>å¯ä»¥å®šä¹‰å¤šä¸ª<code>Trainer</code>å®ä¾‹å¯¹ä¸åŒçš„æ¨¡å‹å‚æ•°ä½¿ç”¨ä¸åŒçš„è¿­ä»£æ–¹æ³•ã€‚</li>
<li>ä¸¢å¼ƒæ³•åªåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨ã€‚</li>
</ul>



<!-- Comments -->
<div>
    
    <hr />
    <h3>Comments:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



</div>



    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });
      </script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>




            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <h2>About</h2>
            <div>
                This blog is used to save the things that I have learnt from Machine Learning and Deep Learning.
            </div>
        </section>
        <section>
            <h2>Follow</h2>
            <ul class="icons">
                
                
                
                
                
                    <li><a href="https://github.com/yeliyang/" class="icon style2 fa-github" target="_blank" ><span class="label">GitHub</span></a></li>
                
                
                
                
                
                    <li><a href="\#" class="icon style2 fa-envelope-o" target="_blank" ><span class="label">Email</span></a></li>
                
                
                    <li><a href="\#" class="icon style2 fa-rss" target="_blank" ><span class="label">RSS</span></a></li>
                
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; Untitled. All rights reserved</li>
            <li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
            <li>Hexo: <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- skel -->

<script src="/js/skel.min.js"></script>


<!-- Custom Code -->

<script src="/js/util.js"></script>


<!--[if lte IE 8]>

<script src="/js/ie/respond.min.js"></script>

<![endif]-->

<!-- Custom Code -->

<script src="/js/main.js"></script>


<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'liyang';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>


</body>

</html>